{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFWUNDvlXfMK",
        "outputId": "98114e48-e8a9-4640-9eb2-11c001db2967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.0\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.0\n",
            "    Uninstalling numpy-2.3.0:\n",
            "      Successfully uninstalled numpy-2.3.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y gensim numpy\n",
        "!pip install numpy --upgrade\n",
        "!pip install gensim --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIPCFXGzc-Df",
        "outputId": "dac207f2-b0b4-4da6-ed93-ff54e36e2047"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "df = pd.read_csv(path + \"/IMDB Dataset.csv\")\n",
        "Df = df.sample(frac=0.2,random_state=42)\n",
        "print(len(Df))\n",
        "print(Df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6GrLBnoXrIb",
        "outputId": "7638f6be-8df3-4b3b-d182-56def0941d82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "                                                  review sentiment\n",
            "33553  I really liked this Summerslam due to the look...  positive\n",
            "9427   Not many television shows appeal to quite as m...  positive\n",
            "199    The film quickly gets to a major chase scene w...  negative\n",
            "12447  Jane Austen would definitely approve of this o...  positive\n",
            "39489  Expectations were somewhat high for me when I ...  negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MEpDe0FYaO-",
        "outputId": "2051ed93-5f83-467c-a22c-b1268d065ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Df.replace({\"sentiment\": {\"positive\": 1, \"negative\": 0}}, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uicv10-gY-_2",
        "outputId": "f7ee4702-d7d3-44f5-a667-4ad0fe1f9df4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2fe096a6c6d4>:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  Df.replace({\"sentiment\": {\"positive\": 1, \"negative\": 0}}, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(Df,test_size=0.2,random_state=42)\n",
        "print(train_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVqm579Ka7_4",
        "outputId": "b3542f22-1e8d-4bd9-9cef-e898c713b589"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing data"
      ],
      "metadata": {
        "id": "mcAJOhSrcAmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSwN9pXBd-O5",
        "outputId": "f6c54dc6-ae1c-46c5-d021-9c0b3cef68bb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "YQ0i2HSXiBmc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [stemmer.stem(token) for token in tokens]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "x_1gYxTqbthL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {'<UNK>':0}\n",
        "\n",
        "def build_vocab(reviews):\n",
        "  for review in reviews:\n",
        "    tokens = preprocess(review)\n",
        "    for token in tokens:\n",
        "      if token not in vocab:\n",
        "        vocab[token] = len(vocab)\n",
        "\n",
        "build_vocab(train_data['review'])"
      ],
      "metadata": {
        "id": "kwftjA2aiN8S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(text,vocab):\n",
        "  indexed_text = []\n",
        "  for token in preprocess(text):\n",
        "    if token in vocab:\n",
        "      indexed_text.append(vocab[token])\n",
        "    else:\n",
        "      indexed_text.append(vocab['<UNK>'])\n",
        "  return indexed_text"
      ],
      "metadata": {
        "id": "T4VdBAi_jIcE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM - Long Short Term Memory"
      ],
      "metadata": {
        "id": "VdxJiNt8gkHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "\n",
        "  def __init__(self,data,vocab):\n",
        "    self.data = data\n",
        "    self.vocab = vocab\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    row = self.data.iloc[idx]\n",
        "    review = text_to_indices(row['review'],self.vocab)\n",
        "    label = row['sentiment']\n",
        "    return torch.tensor(review), torch.tensor(label)"
      ],
      "metadata": {
        "id": "LX8fntXYf9Uk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = IMDBDataset(train_data,vocab)\n",
        "test_dataset = IMDBDataset(test_data,vocab)"
      ],
      "metadata": {
        "id": "3icyB_Dqh42I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_dataset[0]))\n",
        "print(type(train_dataset[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDE0Hy17qfuD",
        "outputId": "bd601d1d-4158-4dd3-e2a7-dc63f643e4d9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tuple'>\n",
            "<class 'tuple'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    reviews, labels = zip(*batch)\n",
        "    lengths = torch.tensor([len(rev) for rev in reviews])\n",
        "    padded_reviews = pad_sequence(reviews, batch_first=True)\n",
        "    labels = torch.stack(labels)\n",
        "    return padded_reviews, labels, lengths"
      ],
      "metadata": {
        "id": "hprSbNMfmxtX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset,batch_size=32,shuffle=True,collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset,batch_size=32,shuffle=False,collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "t9Ti84tTiHsG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
      ],
      "metadata": {
        "id": "E09SczXvnkBY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBnn(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,embedding_dim=100)\n",
        "    self.lstm = nn.LSTM(100,64,batch_first=True)\n",
        "    self.linear = nn.Linear(64,1)\n",
        "\n",
        "  def forward(self,review,lengths):\n",
        "    embedded = self.embedding(review)\n",
        "    lengths_sorted, idx_sort = torch.sort(lengths, descending=True)\n",
        "    embedded_sorted = embedded[idx_sort]\n",
        "\n",
        "    packed = pack_padded_sequence(\n",
        "        embedded_sorted,\n",
        "        lengths_sorted.cpu(),\n",
        "        batch_first=True,\n",
        "        enforce_sorted=True\n",
        "    )\n",
        "\n",
        "    output, (h_n, _) = self.lstm(packed)\n",
        "\n",
        "    _, idx_unsort = torch.sort(idx_sort)\n",
        "    h_n = h_n[:, idx_unsort, :]\n",
        "\n",
        "    out = self.linear(h_n.squeeze(0))\n",
        "    return out"
      ],
      "metadata": {
        "id": "jWfhByl3iUDw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = IMDBnn(len(vocab)).to(device)"
      ],
      "metadata": {
        "id": "J2oSUBVpn-qn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.001\n",
        "epochs = 25"
      ],
      "metadata": {
        "id": "SsHc0QBMm4pw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimiser = torch.optim.Adam(model.parameters(),lr=lr)"
      ],
      "metadata": {
        "id": "2g6O_IN4nr2V"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "\n",
        "print_interval = 100\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0;\n",
        "  for batch_idx, (reviews, labels, lengths) in enumerate(train_loader):\n",
        "    reviews, labels = reviews.to(device), labels.to(device).float()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(reviews, lengths)\n",
        "\n",
        "    # Loss calculation\n",
        "    loss = criterion(outputs.squeeze(1), labels)\n",
        "    total_loss += loss.item()\n",
        "    if (batch_idx + 1) % print_interval == 0:\n",
        "      avg_loss = total_loss / print_interval\n",
        "      print(f\"Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {avg_loss:.4f}\")\n",
        "      total_loss = 0\n",
        "\n",
        "    # Backward pass and update\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()"
      ],
      "metadata": {
        "id": "N4Lm3tPmpGGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe45cdd7-b4de-47ba-8cbd-f83f99b0d1e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 100, Loss: 0.6904\n",
            "Epoch: 1, Batch: 200, Loss: 0.6611\n",
            "Epoch: 2, Batch: 100, Loss: 0.5925\n",
            "Epoch: 2, Batch: 200, Loss: 0.5480\n",
            "Epoch: 3, Batch: 100, Loss: 0.4935\n",
            "Epoch: 3, Batch: 200, Loss: 0.4342\n",
            "Epoch: 4, Batch: 100, Loss: 0.3760\n",
            "Epoch: 4, Batch: 200, Loss: 0.3553\n",
            "Epoch: 5, Batch: 100, Loss: 0.3446\n",
            "Epoch: 5, Batch: 200, Loss: 0.3028\n",
            "Epoch: 6, Batch: 100, Loss: 0.2219\n",
            "Epoch: 6, Batch: 200, Loss: 0.2579\n",
            "Epoch: 7, Batch: 100, Loss: 0.1824\n",
            "Epoch: 7, Batch: 200, Loss: 0.1617\n",
            "Epoch: 8, Batch: 100, Loss: 0.2077\n",
            "Epoch: 8, Batch: 200, Loss: 0.2185\n",
            "Epoch: 9, Batch: 100, Loss: 0.1353\n",
            "Epoch: 9, Batch: 200, Loss: 0.1265\n",
            "Epoch: 10, Batch: 100, Loss: 0.2125\n",
            "Epoch: 10, Batch: 200, Loss: 0.1105\n",
            "Epoch: 11, Batch: 100, Loss: 0.0957\n",
            "Epoch: 11, Batch: 200, Loss: 0.0666\n",
            "Epoch: 12, Batch: 100, Loss: 0.0434\n",
            "Epoch: 12, Batch: 200, Loss: 0.0487\n",
            "Epoch: 13, Batch: 100, Loss: 0.0381\n",
            "Epoch: 13, Batch: 200, Loss: 0.0453\n",
            "Epoch: 14, Batch: 100, Loss: 0.0186\n",
            "Epoch: 14, Batch: 200, Loss: 0.0542\n",
            "Epoch: 15, Batch: 100, Loss: 0.0229\n",
            "Epoch: 15, Batch: 200, Loss: 0.0127\n",
            "Epoch: 16, Batch: 100, Loss: 0.0309\n",
            "Epoch: 16, Batch: 200, Loss: 0.0320\n",
            "Epoch: 17, Batch: 100, Loss: 0.0128\n",
            "Epoch: 17, Batch: 200, Loss: 0.0102\n",
            "Epoch: 18, Batch: 100, Loss: 0.0092\n",
            "Epoch: 18, Batch: 200, Loss: 0.0061\n",
            "Epoch: 19, Batch: 100, Loss: 0.0061\n",
            "Epoch: 19, Batch: 200, Loss: 0.0047\n",
            "Epoch: 20, Batch: 100, Loss: 0.0015\n",
            "Epoch: 20, Batch: 200, Loss: 0.0195\n",
            "Epoch: 21, Batch: 100, Loss: 0.1554\n",
            "Epoch: 21, Batch: 200, Loss: 0.0988\n",
            "Epoch: 22, Batch: 100, Loss: 0.0440\n",
            "Epoch: 22, Batch: 200, Loss: 0.0346\n",
            "Epoch: 23, Batch: 100, Loss: 0.0112\n",
            "Epoch: 23, Batch: 200, Loss: 0.0121\n",
            "Epoch: 24, Batch: 100, Loss: 0.0157\n",
            "Epoch: 24, Batch: 200, Loss: 0.0081\n",
            "Epoch: 25, Batch: 100, Loss: 0.0084\n",
            "Epoch: 25, Batch: 200, Loss: 0.0063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaluation\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for reviews, labels, lengths in test_loader:\n",
        "    reviews, labels = reviews.to(device), labels.to(device).float()\n",
        "    pred_val = model(reviews, lengths)\n",
        "    pred_val = pred_val.squeeze(1)\n",
        "    y_cap = np.where(pred_val.cpu().detach().numpy() >= 0.5, 1, 0)\n",
        "    labels_np = labels.cpu().detach().numpy()\n",
        "    correct += np.sum(y_cap == labels_np)\n",
        "    total += len(labels)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdQrBjEhq7R5",
        "outputId": "8402fa81-1200-4ccd-e3d9-0cf826c47980"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8030\n"
          ]
        }
      ]
    }
  ]
}